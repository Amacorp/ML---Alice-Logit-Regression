{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/test_sessions.csv\n",
      "/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/train.zip\n",
      "/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/site_dic.pkl\n",
      "/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/sample_submission.csv\n",
      "/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/train_sessions.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**Intro**\n",
    "\n",
    "В рамках этого соревнования мы будем решать проблему обнаружения злоумышленника, анализируя его поведение. Это сложная и интересная проблема, сочетающая анализ данных и поведенческую психологию.\n",
    "\n",
    "Например: Яндекс решает проблему обнаружения вторжений в почтовые ящики на основе моделей поведения пользователя. Короче говоря, модель поведения нарушителя может отличаться от модели владельца:\n",
    "\n",
    "- нарушитель может не удалять электронные письма сразу после их прочтения, как это делает владелец почтового ящика;\n",
    "- злоумышленник может помечать электронные письма и даже перемещать курсор по-другому\n",
    "- и т.п.\n",
    "\n",
    "Таким образом, злоумышленник может быть обнаружен и выброшен из почтового ящика. При этом владельцу может быть предложено пройти аутентификацию с помощью SMS-кода. Такой пилотный проект Яндекса описан в статье Хабре (https://habr.com/ru/company/yandex/blog/230583/).\n",
    "\n",
    "Подобные вещи разрабатываются в Google Analytics и описываются в научных исследованиях. Вы можете найти более подробную информацию по этой теме, выполнив поиск по запросу «Traversal Pattern Mining» и «Sequential Pattern Mining».\n",
    "\n",
    "В этом конкурсе мы собираемся решить аналогичную проблему: наш алгоритм должен проанализировать последовательность веб-сайтов, которые посещает конкретный пользователь, и спрогнозировать, является ли этот человек Алисой или это злоумышленником (тот,кто не является Алисов). В качестве метрики мы будем использовать метрику ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**Data Downloading and Transformation**\n",
    "\n",
    "Загрузим тренировочный и тестовый наборы и проанализируем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/train_sessions.csv',\n",
    "                      index_col='session_id')\n",
    "test_df = pd.read_csv('/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/test_sessions.csv',\n",
    "                     index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                time6    site7  \\\n",
       "session_id                       ...                                 \n",
       "1                           NaN  ...                  NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...  2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...  2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...  2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...  2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 253561 entries, 1 to 253561\n",
      "Data columns (total 21 columns):\n",
      "site1     253561 non-null int64\n",
      "time1     253561 non-null object\n",
      "site2     250098 non-null float64\n",
      "time2     250098 non-null object\n",
      "site3     246919 non-null float64\n",
      "time3     246919 non-null object\n",
      "site4     244321 non-null float64\n",
      "time4     244321 non-null object\n",
      "site5     241829 non-null float64\n",
      "time5     241829 non-null object\n",
      "site6     239495 non-null float64\n",
      "time6     239495 non-null object\n",
      "site7     237297 non-null float64\n",
      "time7     237297 non-null object\n",
      "site8     235224 non-null float64\n",
      "time8     235224 non-null object\n",
      "site9     233084 non-null float64\n",
      "time9     233084 non-null object\n",
      "site10    231052 non-null float64\n",
      "time10    231052 non-null object\n",
      "target    253561 non-null int64\n",
      "dtypes: float64(9), int64(2), object(10)\n",
      "memory usage: 42.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Набор обучающих данных содержит следующие атрибуты:\n",
    "\n",
    "- **site1** – идентификатор первого посещенного сайта в сеансе\n",
    "- **time1** – время посещения первого сайта в сеансе\n",
    "- ...\n",
    "- **site10** – идентификатор десятого посещенного сайта в сеансе\n",
    "- **time10** – время посещения десятого сайта в сессии\n",
    "- **target** – целевая переменная, имеет значение 1 для сеансов Алисы и 0 для сеансов других пользователей\n",
    "\n",
    "Пользовательские сессии выбираются таким образом, чтобы они не превышали полчаса и/или содержали более десяти веб-сайтов. То есть сеанс считается завершенным, если пользователь посетил десять веб-сайтов или если сеанс длился более тридцати минут.\n",
    "\n",
    "В таблице есть несколько пустых значений, это означает, что некоторые сеансы содержат менее десяти веб-сайтов. Замените пустые значения на 0 и измените типы столбцов на целочисленные значения. Также загрузите словарь веб-сайтов и проверьте, как он выглядит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Меняем тип атрибутов site1, ..., site10 на целочисленный и заменяем отсутствующие значения нулями\n",
    "sites = ['site%s' % i for i in range(1,11)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype(int)\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2  site3  \\\n",
       "session_id                                                                  \n",
       "1             718  2014-02-20 10:02:45      0                  NaN      0   \n",
       "2             890  2014-02-22 11:19:50    941  2014-02-22 11:19:50   3847   \n",
       "3           14769  2013-12-16 16:40:17     39  2013-12-16 16:40:18  14768   \n",
       "4             782  2014-03-28 10:52:12    782  2014-03-28 10:52:42    782   \n",
       "5              22  2014-02-28 10:53:05    177  2014-02-28 10:55:22    175   \n",
       "\n",
       "                          time3  site4                time4  site5  \\\n",
       "session_id                                                           \n",
       "1                           NaN      0                  NaN      0   \n",
       "2           2014-02-22 11:19:51    941  2014-02-22 11:19:51    942   \n",
       "3           2013-12-16 16:40:19  14769  2013-12-16 16:40:19     37   \n",
       "4           2014-03-28 10:53:12    782  2014-03-28 10:53:42    782   \n",
       "5           2014-02-28 10:55:22    178  2014-02-28 10:55:23    177   \n",
       "\n",
       "                          time5  ...                time6  site7  \\\n",
       "session_id                       ...                               \n",
       "1                           NaN  ...                  NaN      0   \n",
       "2           2014-02-22 11:19:51  ...  2014-02-22 11:19:51   3847   \n",
       "3           2013-12-16 16:40:19  ...  2013-12-16 16:40:19  14768   \n",
       "4           2014-03-28 10:54:12  ...  2014-03-28 10:54:42    782   \n",
       "5           2014-02-28 10:55:23  ...  2014-02-28 10:55:59    175   \n",
       "\n",
       "                          time7  site8                time8  site9  \\\n",
       "session_id                                                           \n",
       "1                           NaN      0                  NaN      0   \n",
       "2           2014-02-22 11:19:52   3846  2014-02-22 11:19:52   1516   \n",
       "3           2013-12-16 16:40:20  14768  2013-12-16 16:40:21  14768   \n",
       "4           2014-03-28 10:55:12    782  2014-03-28 10:55:42    782   \n",
       "5           2014-02-28 10:55:59    177  2014-02-28 10:55:59    177   \n",
       "\n",
       "                          time9 site10               time10 target  \n",
       "session_id                                                          \n",
       "1                           NaN      0                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Вместе с набором данных поставляется словарь сайтов. Посмотрим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/kaggle/input/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25075</th>\n",
       "      <td>www.abmecatronique.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>groups.live.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42436</th>\n",
       "      <td>majeureliguefootball.wordpress.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>cdt46.media.tourinsoft.eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>www.hdwallpapers.eu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     site\n",
       "25075              www.abmecatronique.com\n",
       "13997                     groups.live.com\n",
       "42436  majeureliguefootball.wordpress.com\n",
       "30911           cdt46.media.tourinsoft.eu\n",
       "8104                  www.hdwallpapers.eu"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48371, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 21), (82797, 20))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Для самой базовой модели мы будем использовать только посещенные веб-сайты в сеансе (но мы не будем учитывать атрибуты меток времени). Смысл такого выбора: у Алисы есть свои любимые сайты, и чем чаще вы видите эти сайты в сеансе, тем выше вероятность того, что это сеанс Алисы, и наоборот.\n",
    "\n",
    "Давайте подготовим данные, мы возьмем только атрибуты `site1, site2, ..., site10` из всего датасета. Имейте в виду, что пропущенные значения заменяются на ноль.\n",
    "\n",
    "Преобразуем данные в формат, который можно передать в `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train_df.shape[0]\n",
    "data = pd.concat([train_df, test_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[sites].to_csv('data_sessions_text.txt', \n",
    "                                 sep=' ', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718 0 0 0 0 0 0 0 0 0\r\n",
      "890 941 3847 941 942 3846 3847 3846 1516 1518\r\n",
      "14769 39 14768 14769 37 39 14768 14768 14768 14768\r\n",
      "782 782 782 782 782 782 782 782 782 782\r\n",
      "22 177 175 178 177 178 175 177 177 178\r\n",
      "570 21 570 21 21 0 0 0 0 0\r\n",
      "803 23 5956 17513 37 21 803 17514 17514 17514\r\n",
      "22 21 29 5041 14422 23 21 5041 14421 14421\r\n",
      "668 940 942 941 941 942 940 23 21 22\r\n",
      "3700 229 570 21 229 21 21 21 2336 2044\r\n"
     ]
    }
   ],
   "source": [
    "!head data_sessions_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 1), max_features=50000)\n",
    "with open('data_sessions_text.txt') as inp_file:\n",
    "    data = cv.fit_transform(inp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 48362) (82797, 48362)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:idx]\n",
    "X_test = data[idx:]\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**Training the first model**\n",
    "\n",
    "Итак, у нас есть алгоритм и данные для него. Давайте построим первую модель, используя реализацию логистической регрессии в Scikit-learn с параметрами по умолчанию. Для оценивания модели мы будем использовать отложенную выборка размера 1/10 от тренировочного набора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "log_reg = LogisticRegression(C=1.0, random_state=42, solver='lbfgs', max_iter=500)\n",
    "X_train_log, X_valid_log, y_train_log, y_valid_log = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "log_reg.fit(X_train_log, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9613767578231046"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_reg.predict_proba(X_valid_log)\n",
    "score = roc_auc_score(y_valid_log, y_pred[:,1])\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Первая модель продемонстрировала качество 0,96. Давайте возьмем это значение в качестве отправной точки. Чтобы сделать прогноз на тестовом наборе, нужно снова обучить модель на всем тренировочном (до этого момента модель использовала только часть данных для обучения), что увеличит ее обобщающую способность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем предсказания\n",
    "y_test = log_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97690446e-01, 2.30955385e-03],\n",
       "       [9.99999995e-01, 4.61401495e-09],\n",
       "       [9.99999981e-01, 1.89667578e-08],\n",
       "       [9.99999972e-01, 2.75422934e-08],\n",
       "       [9.99967356e-01, 3.26442327e-05]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(y_test[:,1], 'baseline_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'penalty' : ['l1', 'l2'],\n",
    "    'C' : [0.001,0.01,0.1,1,10,100,1000],\n",
    "    'solver' : ['liblinear']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=500, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=42, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(log_reg, scoring = 'roc_auc', param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = grid_search.best_estimator_.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97570132e-01, 2.42986817e-03],\n",
       "       [9.99999995e-01, 5.10599228e-09],\n",
       "       [9.99999981e-01, 1.88419076e-08],\n",
       "       ...,\n",
       "       [9.91156738e-01, 8.84326233e-03],\n",
       "       [9.99525444e-01, 4.74556322e-04],\n",
       "       [9.99977370e-01, 2.26304794e-05]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(y_test[:,1], 'baseline_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
